{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 10:15:35.836657: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-22 10:15:35.843382: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-22 10:15:36.227857: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-22 10:15:38.390044: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-22 10:15:40.938798: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from IPython.display import display\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.16.1\n",
      "Using GPU/CPU\n",
      "Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../raw_data/AI4Code\"\n",
    "BASE_MODEL = \"../models/distilbert-base-uncased\"\n",
    "N_SPLITS = 5\n",
    "SEQ_LEN = 128\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "try:\n",
    "    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(TPU)\n",
    "    tf.tpu.experimental.initialize_tpu_system(TPU)\n",
    "    STRATEGY = tf.distribute.experimental.TPUStrategy(TPU)\n",
    "    BATCH_SIZE = 128 * STRATEGY.num_replicas_in_sync\n",
    "except Exception:\n",
    "    TPU = None\n",
    "    STRATEGY = tf.distribute.get_strategy()\n",
    "    BATCH_SIZE = 32\n",
    "    LIMIT = 10_000\n",
    "\n",
    "print(\"TensorFlow\", tf.__version__)\n",
    "\n",
    "if TPU is not None:\n",
    "    print(\"Using TPU v3-8\")\n",
    "else:\n",
    "    print(\"Using GPU/CPU\")\n",
    "\n",
    "print(\"Batch size:\", BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_notebook(path: str) -> pd.DataFrame:\n",
    "    with open(path) as file:\n",
    "        df = pd.DataFrame(json.load(file))\n",
    "    df[\"id\"] = os.path.splitext(os.path.basename(path))[0]\n",
    "    return df\n",
    "\n",
    "\n",
    "def expand_order(row: Tuple[str, str]) -> pd.DataFrame:\n",
    "    cell_ids = row[1].split(\" \")\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [row[0] for _ in range(len(cell_ids))],\n",
    "            \"cell_id\": cell_ids,\n",
    "            \"rank\": range(len(cell_ids)),\n",
    "        }\n",
    "    )\n",
    "    df[\"pct_rank\"] = df[\"rank\"] / len(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def tokenize(source: pd.Series) -> Tuple[np.array, np.array]:\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(BASE_MODEL, do_lower_case=True)\n",
    "\n",
    "    input_ids = np.zeros((len(source), SEQ_LEN), dtype=\"int32\")\n",
    "    attention_mask = np.zeros((len(source), SEQ_LEN), dtype=\"int32\")\n",
    "\n",
    "    for i, x in enumerate(tqdm(source, total=len(source))):\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            x,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=SEQ_LEN,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        input_ids[i] = encoding[\"input_ids\"]\n",
    "        attention_mask[i] = encoding[\"attention_mask\"]\n",
    "\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    input_ids: np.array,\n",
    "    attention_mask: np.array,\n",
    "    labels: Optional[np.array] = None,\n",
    "    ordered: bool = False,\n",
    "    repeated: bool = False,\n",
    ") -> tf.data.Dataset:\n",
    "    if labels is not None:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            ({\"input_ids\": input_ids, \"attention_mask\": attention_mask}, labels)\n",
    "        )\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "        )\n",
    "    if repeated:\n",
    "        dataset = dataset.repeat()\n",
    "    if not ordered:\n",
    "        dataset = dataset.shuffle(1024)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_model() -> tf.keras.Model:\n",
    "    backbone = transformers.TFDistilBertModel.from_pretrained(BASE_MODEL)\n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(SEQ_LEN,),\n",
    "        dtype=tf.int32,\n",
    "        name=\"input_ids\",\n",
    "    )\n",
    "    attention_mask = tf.keras.layers.Input(\n",
    "        shape=(SEQ_LEN,),\n",
    "        dtype=tf.int32,\n",
    "        name=\"attention_mask\",\n",
    "    )\n",
    "    x = backbone(\n",
    "        {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        },\n",
    "    )\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"linear\", dtype=\"float32\")(x[0][:, 0, :])\n",
    "\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_ids, attention_mask],\n",
    "        outputs=outputs,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b96184f9efb43d1af3d6c6b0a41052f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdccf1eb7b1c4c6fa5fba01e105d1181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>rank</th>\n",
       "      <th>pct_rank</th>\n",
       "      <th>ancestor_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58900633</td>\n",
       "      <td>#### Explore airports\\nThere are 268 unique ai...</td>\n",
       "      <td>2ac1be019bf73e</td>\n",
       "      <td>26</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>b66b5e9a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>950a8058</td>\n",
       "      <td>#### Airlines\\nAfter looking into delay distri...</td>\n",
       "      <td>2ac1be019bf73e</td>\n",
       "      <td>21</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>b66b5e9a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3d3eb7f6</td>\n",
       "      <td>Distribution of airlines is extremely right sk...</td>\n",
       "      <td>2ac1be019bf73e</td>\n",
       "      <td>25</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>b66b5e9a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99385c1e</td>\n",
       "      <td>#### How did carriers performed over the years?</td>\n",
       "      <td>2ac1be019bf73e</td>\n",
       "      <td>52</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>b66b5e9a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26c65ce6</td>\n",
       "      <td>An average departure delay and its std are slo...</td>\n",
       "      <td>2ac1be019bf73e</td>\n",
       "      <td>45</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>b66b5e9a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156707</th>\n",
       "      <td>7fafd119</td>\n",
       "      <td># 라이브러리 &amp; 데이터 불러오기</td>\n",
       "      <td>ff2cfc75d39200</td>\n",
       "      <td>75</td>\n",
       "      <td>0.669643</td>\n",
       "      <td>7cc543d3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156708</th>\n",
       "      <td>e75ce892</td>\n",
       "      <td>## ❔❓ 제품 배송 시간에 맞춰 배송되었는지 예측모델 만들기</td>\n",
       "      <td>ff2cfc75d39200</td>\n",
       "      <td>109</td>\n",
       "      <td>0.973214</td>\n",
       "      <td>7cc543d3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156709</th>\n",
       "      <td>99e99cc4</td>\n",
       "      <td>### 정규화</td>\n",
       "      <td>ff2cfc75d39200</td>\n",
       "      <td>100</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>7cc543d3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156710</th>\n",
       "      <td>c63df0e4</td>\n",
       "      <td># 💓🐨 연습_02 🐨💓</td>\n",
       "      <td>ff2cfc75d39200</td>\n",
       "      <td>108</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>7cc543d3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156711</th>\n",
       "      <td>1c112aaa</td>\n",
       "      <td>## csv 생성 및 확인</td>\n",
       "      <td>ff2cfc75d39200</td>\n",
       "      <td>70</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>7cc543d3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156712 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cell_id                                             source  \\\n",
       "0       58900633  #### Explore airports\\nThere are 268 unique ai...   \n",
       "1       950a8058  #### Airlines\\nAfter looking into delay distri...   \n",
       "2       3d3eb7f6  Distribution of airlines is extremely right sk...   \n",
       "3       99385c1e    #### How did carriers performed over the years?   \n",
       "4       26c65ce6  An average departure delay and its std are slo...   \n",
       "...          ...                                                ...   \n",
       "156707  7fafd119                                 # 라이브러리 & 데이터 불러오기   \n",
       "156708  e75ce892                 ## ❔❓ 제품 배송 시간에 맞춰 배송되었는지 예측모델 만들기   \n",
       "156709  99e99cc4                                            ### 정규화   \n",
       "156710  c63df0e4                                      # 💓🐨 연습_02 🐨💓   \n",
       "156711  1c112aaa                                     ## csv 생성 및 확인   \n",
       "\n",
       "                    id  rank  pct_rank ancestor_id  \n",
       "0       2ac1be019bf73e    26  0.440678    b66b5e9a  \n",
       "1       2ac1be019bf73e    21  0.355932    b66b5e9a  \n",
       "2       2ac1be019bf73e    25  0.423729    b66b5e9a  \n",
       "3       2ac1be019bf73e    52  0.881356    b66b5e9a  \n",
       "4       2ac1be019bf73e    45  0.762712    b66b5e9a  \n",
       "...                ...   ...       ...         ...  \n",
       "156707  ff2cfc75d39200    75  0.669643    7cc543d3  \n",
       "156708  ff2cfc75d39200   109  0.973214    7cc543d3  \n",
       "156709  ff2cfc75d39200   100  0.892857    7cc543d3  \n",
       "156710  ff2cfc75d39200   108  0.964286    7cc543d3  \n",
       "156711  ff2cfc75d39200    70  0.625000    7cc543d3  \n",
       "\n",
       "[156712 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paths = glob.glob(os.path.join(DATA_PATH, \"train_data\", \"*.json\"))\n",
    "if LIMIT is not None:\n",
    "    paths = paths[:LIMIT]\n",
    "\n",
    "source_df = pd.concat([read_notebook(x) for x in tqdm(paths, total=len(paths))])\n",
    "\n",
    "source_df = source_df[source_df[\"cell_type\"] == \"markdown\"]\n",
    "source_df = source_df.drop(\"cell_type\", axis=1)\n",
    "source_df = source_df.rename_axis(\"cell_id\").reset_index()\n",
    "\n",
    "order_df = pd.read_csv(os.path.join(DATA_PATH, \"train_orders.csv\"), index_col=\"id\")\n",
    "order_df = pd.concat(\n",
    "    [expand_order(row) for row in tqdm(order_df.itertuples(), total=len(order_df))]\n",
    ")\n",
    "\n",
    "ancestors_df = pd.read_csv(\n",
    "    os.path.join(DATA_PATH, \"train_ancestors.csv\"),\n",
    "    usecols=[\"id\", \"ancestor_id\"],\n",
    "    index_col=\"id\",\n",
    ")\n",
    "\n",
    "df = source_df.merge(order_df, on=[\"id\", \"cell_id\"]).merge(ancestors_df, on=\"id\")\n",
    "df = df.dropna()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#from transformers import TFDistilBertForSequenceClassification, DistilBertTokenizer, AutoTokenizer\n",
    "\n",
    "#model_name = \"distilbert-base-uncased\"\n",
    "#model = TFDistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../models/distilbert-base-uncased/tokenizer_config.json',\n",
       " '../models/distilbert-base-uncased/special_tokens_map.json',\n",
       " '../models/distilbert-base-uncased/vocab.txt',\n",
       " '../models/distilbert-base-uncased/added_tokens.json',\n",
       " '../models/distilbert-base-uncased/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "model = TFAutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.save_pretrained(BASE_MODEL)\n",
    "tokenizer.save_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bca5ee50e784ce5b0ab2842a920cc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/156712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: (156712, 128)\n",
      "attention_mask: (156712, 128)\n",
      "labels: (156712,)\n",
      "groups: (156712,)\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_mask = tokenize(df[\"source\"])\n",
    "\n",
    "labels = df[\"pct_rank\"].to_numpy()\n",
    "groups = df[\"ancestor_id\"].to_numpy()\n",
    "\n",
    "print(\"input_ids:\", input_ids.shape)\n",
    "print(\"attention_mask:\", attention_mask.shape)\n",
    "print(\"labels:\", labels.shape)\n",
    "print(\"groups:\", groups.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertModel.\n",
      "\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at ../models/distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'tf_distil_bert_model_1' (type TFDistilBertModel).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for input_ids.\n\nCall arguments received by layer 'tf_distil_bert_model_1' (type TFDistilBertModel):\n  • input_ids={'input_ids': '<KerasTensor shape=(None, 128), dtype=int32, sparse=None, name=input_ids>', 'attention_mask': '<KerasTensor shape=(None, 128), dtype=int32, sparse=None, name=attention_mask>'}\n  • attention_mask=None\n  • head_mask=None\n  • inputs_embeds=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     tf\u001b[38;5;241m.\u001b[39mtpu\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39minitialize_tpu_system(TPU)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m STRATEGY\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m---> 11\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m     14\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m get_dataset(\n\u001b[1;32m     15\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids[train_index],\n\u001b[1;32m     16\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask[train_index],\n\u001b[1;32m     17\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels[train_index],\n\u001b[1;32m     18\u001b[0m     repeated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m )\n",
      "Cell \u001b[0;32mIn[3], line 79\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(\n\u001b[1;32m     70\u001b[0m     shape\u001b[38;5;241m=\u001b[39m(SEQ_LEN,),\n\u001b[1;32m     71\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32,\n\u001b[1;32m     72\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     73\u001b[0m )\n\u001b[1;32m     74\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(\n\u001b[1;32m     75\u001b[0m     shape\u001b[38;5;241m=\u001b[39m(SEQ_LEN,),\n\u001b[1;32m     76\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32,\n\u001b[1;32m     77\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     78\u001b[0m )\n\u001b[0;32m---> 79\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)(x[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m0\u001b[39m, :])\n\u001b[1;32m     87\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(\n\u001b[1;32m     88\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m[input_ids, attention_mask],\n\u001b[1;32m     89\u001b[0m     outputs\u001b[38;5;241m=\u001b[39moutputs,\n\u001b[1;32m     90\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/google_ai4_code/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/google_ai4_code/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:436\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m--> 436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m \u001b[43minput_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args_and_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/google_ai4_code/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:561\u001b[0m, in \u001b[0;36minput_processing\u001b[0;34m(func, config, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(v)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not allowed only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is accepted for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mis_tensor(main_input) \u001b[38;5;129;01mor\u001b[39;00m main_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'tf_distil_bert_model_1' (type TFDistilBertModel).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for input_ids.\n\nCall arguments received by layer 'tf_distil_bert_model_1' (type TFDistilBertModel):\n  • input_ids={'input_ids': '<KerasTensor shape=(None, 128), dtype=int32, sparse=None, name=input_ids>', 'attention_mask': '<KerasTensor shape=(None, 128), dtype=int32, sparse=None, name=attention_mask>'}\n  • attention_mask=None\n  • head_mask=None\n  • inputs_embeds=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False"
     ]
    }
   ],
   "source": [
    "input_ids, attention_mask, labels, groups = shuffle(\n",
    "    input_ids, attention_mask, labels, groups, random_state=RANDOM_STATE\n",
    ")\n",
    "kfold = GroupKFold(n_splits=N_SPLITS)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kfold.split(input_ids, labels, groups=groups)):\n",
    "    if TPU is not None:\n",
    "        tf.tpu.experimental.initialize_tpu_system(TPU)\n",
    "\n",
    "    with STRATEGY.scope():\n",
    "        model = get_model()\n",
    "        model.summary()\n",
    "\n",
    "    train_dataset = get_dataset(\n",
    "        input_ids=input_ids[train_index],\n",
    "        attention_mask=attention_mask[train_index],\n",
    "        labels=labels[train_index],\n",
    "        repeated=True,\n",
    "    )\n",
    "    val_dataset = get_dataset(\n",
    "        input_ids=input_ids[val_index],\n",
    "        attention_mask=attention_mask[val_index],\n",
    "        labels=labels[val_index],\n",
    "        ordered=True,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        steps_per_epoch=len(train_index) // BATCH_SIZE,\n",
    "        epochs=1,\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    model.save_weights(f\"model_{i}.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "google_ai4_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
