{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 17:50:00.695890: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-27 17:50:01.635766: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-27 17:50:01.968332: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-06-27 17:50:01.968379: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-06-27 17:50:05.194756: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-27 17:50:05.195645: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-27 17:50:05.195664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SLICES = 8\n",
    "MD_MAX_LEN = 64\n",
    "TOTAL_MAX_LEN = 512\n",
    "STRATEGY = tf.distribute.get_strategy()\n",
    "BASE_MODEL = \"../models/codebert-base\"\n",
    "TOKENIZER = transformers.AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "INPUT_PATH = \"../raw_data/AI4Code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_notebook(path: str) -> pd.DataFrame:\n",
    "    return (\n",
    "        pd.read_json(path, dtype={\"cell_type\": \"category\", \"source\": \"str\"})\n",
    "        .assign(id=os.path.basename(path).split(\".\")[0])\n",
    "        .rename_axis(\"cell_id\")\n",
    "    )\n",
    "\n",
    "\n",
    "def clean_code(cell: str) -> str:\n",
    "    return str(cell).replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "\n",
    "def sample_cells(cells: List[str], n: int) -> List[str]:\n",
    "    cells = [clean_code(cell) for cell in cells]\n",
    "    if n >= len(cells):\n",
    "        return cells\n",
    "    else:\n",
    "        results = []\n",
    "        step = len(cells) / n\n",
    "        idx = 0\n",
    "        while int(np.round(idx)) < len(cells):\n",
    "            results.append(cells[int(np.round(idx))])\n",
    "            idx += step\n",
    "        if cells[-1] not in results:\n",
    "            results[-1] = cells[-1]\n",
    "        return results\n",
    "\n",
    "\n",
    "def get_features(df: pd.DataFrame) -> dict:\n",
    "    features = {}\n",
    "    for i, sub_df in tqdm(df.groupby(\"id\"), desc=\"Features\"):\n",
    "        features[i] = {}\n",
    "        total_md = sub_df[sub_df.cell_type == \"markdown\"].shape[0]\n",
    "        code_sub_df = sub_df[sub_df.cell_type == \"code\"]\n",
    "        total_code = code_sub_df.shape[0]\n",
    "        codes = sample_cells(code_sub_df.source.values, 20)\n",
    "        features[i][\"total_code\"] = total_code\n",
    "        features[i][\"total_md\"] = total_md\n",
    "        features[i][\"codes\"] = codes\n",
    "    return features\n",
    "\n",
    "\n",
    "def tokenize(df: pd.DataFrame, fts: dict) -> dict:\n",
    "    input_ids = np.zeros((len(df), TOTAL_MAX_LEN), dtype=np.int32)\n",
    "    attention_mask = np.zeros((len(df), TOTAL_MAX_LEN), dtype=np.int32)\n",
    "    features = np.zeros((len(df),), dtype=np.float32)\n",
    "\n",
    "    for i, row in tqdm(\n",
    "        df.reset_index(drop=True).iterrows(), desc=\"Tokens\", total=len(df)\n",
    "    ):\n",
    "        row_fts = fts[row.id]\n",
    "\n",
    "        inputs = TOKENIZER.encode_plus(\n",
    "            row.source,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MD_MAX_LEN,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        code_inputs = TOKENIZER.batch_encode_plus(\n",
    "            [str(x) for x in row_fts[\"codes\"]] or [\"\"],\n",
    "            add_special_tokens=True,\n",
    "            max_length=23,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        for x in code_inputs[\"input_ids\"]:\n",
    "            ids.extend(x[:-1])\n",
    "        ids = ids[:TOTAL_MAX_LEN]\n",
    "        if len(ids) != TOTAL_MAX_LEN:\n",
    "            ids = ids + [\n",
    "                TOKENIZER.pad_token_id,\n",
    "            ] * (TOTAL_MAX_LEN - len(ids))\n",
    "\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        for x in code_inputs[\"attention_mask\"]:\n",
    "            mask.extend(x[:-1])\n",
    "        mask = mask[:TOTAL_MAX_LEN]\n",
    "        if len(mask) != TOTAL_MAX_LEN:\n",
    "            mask = mask + [\n",
    "                TOKENIZER.pad_token_id,\n",
    "            ] * (TOTAL_MAX_LEN - len(mask))\n",
    "\n",
    "        input_ids[i] = ids\n",
    "        attention_mask[i] = mask\n",
    "        features[i] = (\n",
    "            row_fts[\"total_md\"] / (row_fts[\"total_md\"] + row_fts[\"total_code\"]) or 1\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"features\": features,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_ranks(base: pd.Series, derived: List[str]) -> List[str]:\n",
    "    return [base.index(d) for d in derived]\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    input_ids: np.array,\n",
    "    attention_mask: np.array,\n",
    "    feature: np.array,\n",
    ") -> tf.data.Dataset:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"feature\": feature}\n",
    "    )\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "def get_model() -> tf.keras.Model:\n",
    "    backbone = transformers.TFAutoModel.from_pretrained(BASE_MODEL)\n",
    "    input_ids = tf.keras.layers.Input(\n",
    "        shape=(TOTAL_MAX_LEN,),\n",
    "        dtype=tf.int32,\n",
    "        name=\"input_ids\",\n",
    "    )\n",
    "    attention_mask = tf.keras.layers.Input(\n",
    "        shape=(TOTAL_MAX_LEN,),\n",
    "        dtype=tf.int32,\n",
    "        name=\"attention_mask\",\n",
    "    )\n",
    "    feature = tf.keras.layers.Input(\n",
    "        shape=(1,),\n",
    "        dtype=tf.float32,\n",
    "        name=\"feature\",\n",
    "    )\n",
    "    x = backbone({\"input_ids\": input_ids, \"attention_mask\": attention_mask})[0]\n",
    "    x = tf.concat([x[:, 0, :], feature], axis=1)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"linear\", dtype=\"float32\")(x)\n",
    "    return tf.keras.Model(\n",
    "        inputs=[input_ids, attention_mask, feature],\n",
    "        outputs=outputs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob(os.path.join(INPUT_PATH, \"test\", \"*.json\"))\n",
    "df = (\n",
    "    pd.concat([read_notebook(x) for x in tqdm(paths, desc=\"Concat\")])\n",
    "    .set_index(\"id\", append=True)\n",
    "    .swaplevel()\n",
    "    .sort_index(level=\"id\", sort_remaining=False)\n",
    ").reset_index()\n",
    "df[\"source\"] = df[\"source\"].str.slice(0, MD_MAX_LEN)\n",
    "df[\"rank\"] = df.groupby([\"id\", \"cell_type\"]).cumcount()\n",
    "df[\"pct_rank\"] = df.groupby([\"id\", \"cell_type\"])[\"rank\"].rank(pct=True)\n",
    "\n",
    "fts = get_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "google_ai4_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
